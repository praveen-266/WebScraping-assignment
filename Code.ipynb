{"cells":[{"cell_type":"code","execution_count":null,"id":"97f17b74","metadata":{"id":"97f17b74"},"outputs":[],"source":["import re\n","import math as mt \n","import pandas as pd\n","import requests \n","from bs4 import BeautifulSoup \n","from urllib.request import urlopen\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"id":"41b9b762","metadata":{"id":"41b9b762","outputId":"15b802cc-8f0b-40cd-cedd-612792e75656"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>URL_ID</th>\n","      <th>URL</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37</td>\n","      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>38</td>\n","      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>39</td>\n","      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   URL_ID                                                URL\n","0      37  https://insights.blackcoffer.com/ai-in-healthc...\n","1      38  https://insights.blackcoffer.com/what-if-the-c...\n","2      39  https://insights.blackcoffer.com/what-jobs-wil..."]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["ar=pd.read_csv(r'C:\\Users\\prave\\Downloads\\Input.xlsx - Sheet1.csv')\n","ar.head(3)"]},{"cell_type":"code","execution_count":null,"id":"3777ebfd","metadata":{"id":"3777ebfd"},"outputs":[],"source":["URLS=ar['URL'].to_list()"]},{"cell_type":"code","execution_count":null,"id":"d1350a09","metadata":{"id":"d1350a09"},"outputs":[],"source":["# extracting title from wesite\n","list_title=[] \n","for u in URLS:\n","        reqs = requests.get(u) # using the BeautifulSoup module\n","        soup = BeautifulSoup(reqs.text, 'html.parser')\n","        \n","        for title in soup.find_all('title'):\n","            list_title.append(title.get_text())"]},{"cell_type":"code","execution_count":null,"id":"4ff2c6b5","metadata":{"id":"4ff2c6b5"},"outputs":[],"source":["# Extracting article text from the website\n","\n","list_text=[]\n","from bs4 import BeautifulSoup\n","import requests\n","for u in URLS:\n","    \n","    r=requests.get(u)\n","    soup=BeautifulSoup(r.content,\"html.parser\")\n","    div_text=soup.find(\"div\",{\"class\":\"td-main-content-wrap td-container-wrap\"}).get_text()\n","    list_text.append(div_text)"]},{"cell_type":"code","execution_count":null,"id":"5ef2f5c9","metadata":{"id":"5ef2f5c9"},"outputs":[],"source":["ar['title']=list_title\n","ar['text']=list_text"]},{"cell_type":"code","execution_count":null,"id":"8a388ab9","metadata":{"id":"8a388ab9","outputId":"bddba2c7-5927-4f82-a0b4-1e8ddbfd6477"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>URL_ID</th>\n","      <th>URL</th>\n","      <th>title</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37</td>\n","      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n","      <td>AI in healthcare to Improve Patient Outcomes |...</td>\n","      <td>\\n\\nHome  What We Think  AI in healthcare to I...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>38</td>\n","      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n","      <td>What if the Creation is Taking Over the Creato...</td>\n","      <td>\\n\\nHome  What We Think  What if the Creation ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>39</td>\n","      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n","      <td>What Jobs Will Robots Take From Humans in The ...</td>\n","      <td>\\n\\nHome  What We Think  What Jobs Will Robots...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   URL_ID                                                URL  \\\n","0      37  https://insights.blackcoffer.com/ai-in-healthc...   \n","1      38  https://insights.blackcoffer.com/what-if-the-c...   \n","2      39  https://insights.blackcoffer.com/what-jobs-wil...   \n","\n","                                               title  \\\n","0  AI in healthcare to Improve Patient Outcomes |...   \n","1  What if the Creation is Taking Over the Creato...   \n","2  What Jobs Will Robots Take From Humans in The ...   \n","\n","                                                text  \n","0  \\n\\nHome  What We Think  AI in healthcare to I...  \n","1  \\n\\nHome  What We Think  What if the Creation ...  \n","2  \\n\\nHome  What We Think  What Jobs Will Robots...  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["ar.head(3)"]},{"cell_type":"code","execution_count":null,"id":"9ad8721e","metadata":{"id":"9ad8721e"},"outputs":[],"source":["# remove second title from the  title column\n","ar['title']=ar['title'].apply(lambda x:x.split('|')[0])\n","\n","# removing before introduction text i.e(header text)\n","ar['text']=ar['text'].apply(lambda x:x.split(\"Introduction\\n\")[-1])\n","\n","# removing footer text\n","ar['text']=ar['text'].apply(lambda x:x.split(\"\\nBlackcoffer Insights\")[0])\n","\n","# some articles are not have \"introduction\" title and directly written contents. \n","# so, for those kind of article we remove header like below\n","ar['text']=ar['text'].apply(lambda x:x.split('FacebookTwitterPinterestWhatsApp\\n\\n\\n\\n\\n\\n\\n')[-1])"]},{"cell_type":"code","execution_count":null,"id":"913c5247","metadata":{"id":"913c5247","outputId":"c3c4b45b-24b2-4c33-ec3d-9c63a95bf5e1"},"outputs":[{"data":{"text/plain":["(114, 4)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["ar.shape"]},{"cell_type":"markdown","id":"7ed402a4","metadata":{"id":"7ed402a4"},"source":["## Cleaning the StopWords"]},{"cell_type":"code","execution_count":null,"id":"a79ec9b1","metadata":{"id":"a79ec9b1"},"outputs":[],"source":["df_stop=ar.copy()"]},{"cell_type":"code","execution_count":null,"id":"220578d1","metadata":{"id":"220578d1"},"outputs":[],"source":["# remove stopwords\n","\n","file=open(r'C:\\users\\prave\\Downloads\\StopWords_GenericLong.txt','r')\n","generic = file.read().split()\n","\n","file=open(r'C:\\users\\prave\\Downloads\\StopWords_Generic.txt','r')\n","gene = file.read().split()\n","\n","file=open(r'C:\\users\\prave\\Downloads\\StopWords_DatesandNumbers.txt','r')\n","dates = file.read().split()\n","\n","file=open(r'C:\\users\\prave\\Downloads\\StopWords_Currencies.txt','r')\n","currency = file.read().split()\n","\n","file=open(r'C:\\users\\prave\\Downloads\\StopWords_Auditor.txt','r')\n","audit = file.read().split()\n","\n","file=open(r'C:\\users\\prave\\Downloads\\StopWords_Names.txt','r')\n","names = file.read().split()\n","\n","file=open(r'C:\\users\\prave\\Downloads\\StopWords_Geographic.txt','r')\n","geo= file.read().split()"]},{"cell_type":"code","execution_count":null,"id":"e9072997","metadata":{"id":"e9072997"},"outputs":[],"source":["# tokenizing the text column\n","\n","def tokenization(text):\n","    text=re.split('\\W+',text)\n","    return text\n","\n","df_stop['tokenized']=df_stop['text'].apply(lambda x:tokenization(x.lower()))"]},{"cell_type":"code","execution_count":null,"id":"cddd6278","metadata":{"id":"cddd6278","outputId":"f497b423-aedf-4d47-f9cc-e71c1e59c9a9"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tokenized</th>\n","      <th>No_generic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[, if, anything, kills, over, 10, million, peo...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[human, minds, a, fascination, in, itself, car...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ai, is, rapidly, evolving, in, the, employmen...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           tokenized  \\\n","0  [, if, anything, kills, over, 10, million, peo...   \n","1  [human, minds, a, fascination, in, itself, car...   \n","2  [ai, is, rapidly, evolving, in, the, employmen...   \n","\n","                                          No_generic  \n","0  [, kills, 10, million, people, decades, highly...  \n","1  [human, minds, fascination, carrying, potentia...  \n","2  [ai, rapidly, evolving, employment, sector, ma...  "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["def remove_StopWords_GenericLong(text):\n","    text=[word for word in text if word not in generic]\n","    return text\n","df_stop['No_generic']=df_stop['tokenized'].apply(lambda x:remove_StopWords_GenericLong(x))\n","df_stop.loc[:,['tokenized','No_generic']].head(3)"]},{"cell_type":"code","execution_count":null,"id":"68962026","metadata":{"id":"68962026","outputId":"6713d38f-0a3f-47c8-c801-50fdad2280ac"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tokenized</th>\n","      <th>No_generic</th>\n","      <th>No_gene</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[, if, anything, kills, over, 10, million, peo...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[human, minds, a, fascination, in, itself, car...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ai, is, rapidly, evolving, in, the, employmen...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           tokenized  \\\n","0  [, if, anything, kills, over, 10, million, peo...   \n","1  [human, minds, a, fascination, in, itself, car...   \n","2  [ai, is, rapidly, evolving, in, the, employmen...   \n","\n","                                          No_generic  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                             No_gene  \n","0  [, kills, 10, million, people, decades, highly...  \n","1  [human, minds, fascination, carrying, potentia...  \n","2  [ai, rapidly, evolving, employment, sector, ma...  "]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["def remove_StopWords_Gene(text):\n","    text=[word for word in text if word not in gene]\n","    return text\n","df_stop['No_gene']=df_stop['No_generic'].apply(lambda x:remove_StopWords_Gene(x))\n","df_stop.loc[:,['tokenized','No_generic','No_gene']].head(3)"]},{"cell_type":"code","execution_count":null,"id":"d2e362ee","metadata":{"id":"d2e362ee","outputId":"82225d68-f624-4815-c8c3-fe8d4abbb233"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No_generic</th>\n","      <th>No_gene</th>\n","      <th>No_dates</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          No_generic  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                             No_gene  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                            No_dates  \n","0  [, kills, 10, million, people, decades, highly...  \n","1  [human, minds, fascination, carrying, potentia...  \n","2  [ai, rapidly, evolving, employment, sector, ma...  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["def remove_StopWords_dates(text):\n","    text=[word for word in text if word not in dates]\n","    return text\n","\n","df_stop['No_dates']=df_stop['No_gene'].apply(lambda x:remove_StopWords_dates(x))\n","df_stop.loc[:,['No_generic','No_gene','No_dates']].head(3)"]},{"cell_type":"code","execution_count":null,"id":"38174546","metadata":{"id":"38174546","outputId":"b5e13579-aa71-4e9e-8b87-8af2a2926cc0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No_generic</th>\n","      <th>No_gene</th>\n","      <th>No_currency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          No_generic  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                             No_gene  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                         No_currency  \n","0  [, kills, 10, million, people, decades, highly...  \n","1  [human, minds, fascination, carrying, potentia...  \n","2  [ai, rapidly, evolving, employment, sector, ma...  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["def remove_StopWords_currency(text):\n","    text=[word for word in text if word not in currency]\n","    return text\n","\n","df_stop['No_currency']=df_stop['No_dates'].apply(lambda x:remove_StopWords_currency(x))\n","df_stop.loc[:,['No_generic','No_gene','No_currency']].head(3)"]},{"cell_type":"code","execution_count":null,"id":"c2fe3062","metadata":{"id":"c2fe3062","outputId":"0df81374-b34d-491d-ceec-a39e7ee5e231"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No_generic</th>\n","      <th>No_currency</th>\n","      <th>No_names</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          No_generic  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                         No_currency  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                            No_names  \n","0  [, kills, 10, million, people, decades, highly...  \n","1  [human, minds, fascination, carrying, potentia...  \n","2  [ai, rapidly, evolving, employment, sector, ma...  "]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["def remove_StopWords_names(text):\n","    text=[word for word in text if word not in names]\n","    return text\n","df_stop['No_names']=df_stop['No_currency'].apply(lambda x:remove_StopWords_names(x))\n","df_stop.loc[:,['No_generic','No_currency','No_names']].head(3)"]},{"cell_type":"code","execution_count":null,"id":"955d8709","metadata":{"id":"955d8709","outputId":"0d2cf65e-2b3b-4ea8-8267-6ca051907ca5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No_generic</th>\n","      <th>No_currency</th>\n","      <th>No_geo</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          No_generic  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                         No_currency  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                              No_geo  \n","0  [, kills, 10, million, people, decades, highly...  \n","1  [human, minds, fascination, carrying, potentia...  \n","2  [ai, rapidly, evolving, employment, sector, ma...  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["def remove_StopWords_geo(text):\n","    text=[word for word in text if word not in geo]\n","    return text\n","\n","df_stop['No_geo']=df_stop['No_names'].apply(lambda x:remove_StopWords_geo(x))\n","df_stop.loc[:,['No_generic','No_currency','No_geo']].head(3)"]},{"cell_type":"code","execution_count":null,"id":"59f9786f","metadata":{"id":"59f9786f","outputId":"03864f20-fa63-4281-ed79-65cc47acd1f4"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No_generic</th>\n","      <th>No_currency</th>\n","      <th>No_audit</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          No_generic  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                         No_currency  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                            No_audit  \n","0  [, kills, 10, million, people, decades, highly...  \n","1  [human, minds, fascination, carrying, potentia...  \n","2  [ai, rapidly, evolving, employment, sector, ma...  "]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["def remove_StopWords_audit(text):\n","    text=[word for word in text if word not in audit]\n","    return text\n","\n","df_stop['No_audit']=df_stop['No_geo'].apply(lambda x:remove_StopWords_audit(x))\n","df_stop.loc[:,['No_generic','No_currency','No_audit']].head(3)"]},{"cell_type":"code","execution_count":null,"id":"9c228a79","metadata":{"id":"9c228a79"},"outputs":[],"source":["df_stop['text']=df_stop['No_audit']"]},{"cell_type":"code","execution_count":null,"id":"b3e4e2fc","metadata":{"id":"b3e4e2fc","outputId":"cf0021b5-f139-4020-f40e-fe00784e0fa0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>URL_ID</th>\n","      <th>URL</th>\n","      <th>title</th>\n","      <th>text</th>\n","      <th>tokenized</th>\n","      <th>No_generic</th>\n","      <th>No_gene</th>\n","      <th>No_dates</th>\n","      <th>No_currency</th>\n","      <th>No_names</th>\n","      <th>No_geo</th>\n","      <th>No_audit</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37</td>\n","      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n","      <td>AI in healthcare to Improve Patient Outcomes</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, if, anything, kills, over, 10, million, peo...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","      <td>[, kills, 10, million, people, decades, highly...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>38</td>\n","      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n","      <td>What if the Creation is Taking Over the Creator?</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, a, fascination, in, itself, car...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","      <td>[human, minds, fascination, carrying, potentia...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>39</td>\n","      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n","      <td>What Jobs Will Robots Take From Humans in The ...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, is, rapidly, evolving, in, the, employmen...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   URL_ID                                                URL  \\\n","0      37  https://insights.blackcoffer.com/ai-in-healthc...   \n","1      38  https://insights.blackcoffer.com/what-if-the-c...   \n","2      39  https://insights.blackcoffer.com/what-jobs-wil...   \n","\n","                                               title  \\\n","0      AI in healthcare to Improve Patient Outcomes    \n","1  What if the Creation is Taking Over the Creator?    \n","2  What Jobs Will Robots Take From Humans in The ...   \n","\n","                                                text  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                           tokenized  \\\n","0  [, if, anything, kills, over, 10, million, peo...   \n","1  [human, minds, a, fascination, in, itself, car...   \n","2  [ai, is, rapidly, evolving, in, the, employmen...   \n","\n","                                          No_generic  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                             No_gene  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                            No_dates  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                         No_currency  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                            No_names  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                              No_geo  \\\n","0  [, kills, 10, million, people, decades, highly...   \n","1  [human, minds, fascination, carrying, potentia...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                            No_audit  \n","0  [, kills, 10, million, people, decades, highly...  \n","1  [human, minds, fascination, carrying, potentia...  \n","2  [ai, rapidly, evolving, employment, sector, ma...  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["df_stop.head(3)"]},{"cell_type":"markdown","id":"223c07b6","metadata":{"id":"223c07b6"},"source":["## Lemmatization"]},{"cell_type":"code","execution_count":null,"id":"b10d833c","metadata":{"id":"b10d833c"},"outputs":[],"source":["df_lem=df_stop.loc[:,['text','tokenized']]"]},{"cell_type":"code","execution_count":null,"id":"2e172e79","metadata":{"id":"2e172e79"},"outputs":[],"source":["import nltk\n","\n","ps=nltk.PorterStemmer()\n","def stemmer(text):\n","    text=[ps.stem(word) for word in text]\n","    return text\n","df_lem['stemmed_porter']=df_lem['text'].apply(lambda x:stemmer(x))\n","\n","from nltk.stem.snowball import SnowballStemmer\n","s_stemmer=SnowballStemmer(language='english')\n","def stemming2(text):\n","    text=[s_stemmer.stem(word) for word in text]\n","    return text\n","df_lem['stemmed_Snowball']=df_lem['text'].apply(lambda x:stemming2(x))\n","\n","wn=nltk.WordNetLemmatizer()\n","def lemmatizer(text):\n","    text=[wn.lemmatize(word) for word in text]\n","    return text\n","df_lem['lemmatized']=df_lem['text'].apply(lambda x:lemmatizer(x))\n"]},{"cell_type":"code","execution_count":null,"id":"a39fc66a","metadata":{"id":"a39fc66a"},"outputs":[],"source":["df_lem['text']=df_lem['lemmatized']"]},{"cell_type":"code","execution_count":null,"id":"26d62556","metadata":{"id":"26d62556"},"outputs":[],"source":["# Removing Empty string \n","\n","df_lem['text'] = df_lem['text'].apply(lambda x:[i for i in x if i])"]},{"cell_type":"code","execution_count":null,"id":"501f2e0f","metadata":{"id":"501f2e0f","outputId":"99c5683d-d19f-4a28-ff30-2409c9901808"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>tokenized</th>\n","      <th>stemmed_porter</th>\n","      <th>stemmed_Snowball</th>\n","      <th>lemmatized</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[kill, 10, million, people, decade, highly, in...</td>\n","      <td>[, if, anything, kills, over, 10, million, peo...</td>\n","      <td>[, kill, 10, million, peopl, decad, highli, in...</td>\n","      <td>[, kill, 10, million, peopl, decad, high, infe...</td>\n","      <td>[, kill, 10, million, people, decade, highly, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[human, mind, fascination, carrying, potential...</td>\n","      <td>[human, minds, a, fascination, in, itself, car...</td>\n","      <td>[human, mind, fascin, carri, potenti, tinker, ...</td>\n","      <td>[human, mind, fascin, carri, potenti, tinker, ...</td>\n","      <td>[human, mind, fascination, carrying, potential...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>[ai, is, rapidly, evolving, in, the, employmen...</td>\n","      <td>[ai, rapidli, evolv, employ, sector, matter, i...</td>\n","      <td>[ai, rapid, evolv, employ, sector, matter, inv...</td>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  \\\n","0  [kill, 10, million, people, decade, highly, in...   \n","1  [human, mind, fascination, carrying, potential...   \n","2  [ai, rapidly, evolving, employment, sector, ma...   \n","\n","                                           tokenized  \\\n","0  [, if, anything, kills, over, 10, million, peo...   \n","1  [human, minds, a, fascination, in, itself, car...   \n","2  [ai, is, rapidly, evolving, in, the, employmen...   \n","\n","                                      stemmed_porter  \\\n","0  [, kill, 10, million, peopl, decad, highli, in...   \n","1  [human, mind, fascin, carri, potenti, tinker, ...   \n","2  [ai, rapidli, evolv, employ, sector, matter, i...   \n","\n","                                    stemmed_Snowball  \\\n","0  [, kill, 10, million, peopl, decad, high, infe...   \n","1  [human, mind, fascin, carri, potenti, tinker, ...   \n","2  [ai, rapid, evolv, employ, sector, matter, inv...   \n","\n","                                          lemmatized  \n","0  [, kill, 10, million, people, decade, highly, ...  \n","1  [human, mind, fascination, carrying, potential...  \n","2  [ai, rapidly, evolving, employment, sector, ma...  "]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["df_lem.head(3)"]},{"cell_type":"markdown","id":"cecb2d91","metadata":{"id":"cecb2d91"},"source":["## Sentiment Scores"]},{"cell_type":"code","execution_count":null,"id":"ee682951","metadata":{"id":"ee682951"},"outputs":[],"source":["df_scores=df_lem.loc[:,['text']]"]},{"cell_type":"code","execution_count":null,"id":"82fa2600","metadata":{"id":"82fa2600"},"outputs":[],"source":["file = open(r'C:\\users\\prave\\Downloads\\negative-words.txt', 'r')\n","neg_words = file.read().split()\n","file = open(r'C:\\users\\prave\\Downloads\\positive-words.txt', 'r')\n","pos_words = file.read().split()"]},{"cell_type":"code","execution_count":null,"id":"1ea6aebd","metadata":{"id":"1ea6aebd","outputId":"7bd90f83-36bd-4d93-a64e-a6940d08ab1a"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[kill, 10, million, people, decade, highly, in...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[human, mind, fascination, carrying, potential...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text\n","0  [kill, 10, million, people, decade, highly, in...\n","1  [human, mind, fascination, carrying, potential...\n","2  [ai, rapidly, evolving, employment, sector, ma..."]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["df_scores.head(3)"]},{"cell_type":"code","execution_count":null,"id":"f0a996c1","metadata":{"id":"f0a996c1"},"outputs":[],"source":["# Positive Score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary\n","# and then adding up all the values.\n","num_pos = df_scores['text'].map(lambda x: len([i for i in x if i in pos_words]))\n","df_scores['pos_score'] = num_pos+1\n","\n","# Negative Score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary \n","# and then adding up all the values. We multiply the score with -1 so that the score is a positive number.\n","num_neg = df_scores['text'].map(lambda x: len([i for i in x if i in neg_words]))\n","df_scores['neg_score'] = num_neg-1*-1"]},{"cell_type":"code","execution_count":null,"id":"df650de0","metadata":{"id":"df650de0","outputId":"2d13a3e1-4ffe-418e-fe51-ef5a7d900b32"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>pos_score</th>\n","      <th>neg_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[kill, 10, million, people, decade, highly, in...</td>\n","      <td>81</td>\n","      <td>38</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[human, mind, fascination, carrying, potential...</td>\n","      <td>78</td>\n","      <td>39</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ai, rapidly, evolving, employment, sector, ma...</td>\n","      <td>70</td>\n","      <td>43</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  pos_score  neg_score\n","0  [kill, 10, million, people, decade, highly, in...         81         38\n","1  [human, mind, fascination, carrying, potential...         78         39\n","2  [ai, rapidly, evolving, employment, sector, ma...         70         43"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["df_scores.loc[:,['text','pos_score','neg_score']].head(3)"]},{"cell_type":"code","execution_count":null,"id":"8e58a950","metadata":{"id":"8e58a950"},"outputs":[],"source":["# converting list string into text\n","\n","df_scores['text']= df_scores['text'].apply(' '.join)"]},{"cell_type":"code","execution_count":null,"id":"43b23587","metadata":{"id":"43b23587","outputId":"76b9743f-61ba-4bbe-d7e5-3323f566aeac"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>pos_score</th>\n","      <th>neg_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>kill 10 million people decade highly infectiou...</td>\n","      <td>81</td>\n","      <td>38</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>human mind fascination carrying potential tink...</td>\n","      <td>78</td>\n","      <td>39</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ai rapidly evolving employment sector matter i...</td>\n","      <td>70</td>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>give rise smarter human intelligence form arti...</td>\n","      <td>75</td>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>machine intelligence invention humanity make n...</td>\n","      <td>68</td>\n","      <td>30</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  pos_score  neg_score\n","0  kill 10 million people decade highly infectiou...         81         38\n","1  human mind fascination carrying potential tink...         78         39\n","2  ai rapidly evolving employment sector matter i...         70         43\n","3  give rise smarter human intelligence form arti...         75         33\n","4  machine intelligence invention humanity make n...         68         30"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["df_scores.head()"]},{"cell_type":"code","execution_count":null,"id":"96b4dd8f","metadata":{"id":"96b4dd8f"},"outputs":[],"source":["# Total words after cleaning\n","\n","df_scores['total_clean_words'] = df_scores['text'].apply(lambda x: len(x.split()))"]},{"cell_type":"code","execution_count":null,"id":"089bc579","metadata":{"id":"089bc579"},"outputs":[],"source":["# Polarity Score = (Positive Score â€“ Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n","\n","df_scores['polarity_score']= (df_scores['pos_score'] - df_scores['neg_score'])/ ((df_scores['pos_score'] + df_scores['neg_score']) + 0.000001)"]},{"cell_type":"code","execution_count":null,"id":"31cdd78f","metadata":{"id":"31cdd78f"},"outputs":[],"source":["# Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n","\n","df_scores['sub_score']=(df_scores['pos_score']+df_scores['neg_score'])/(df_scores['total_clean_words']+ 0.000001)"]},{"cell_type":"code","execution_count":null,"id":"719e617b","metadata":{"id":"719e617b","outputId":"4108ba40-7dab-4fc4-a51b-05811baa9bb0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>pos_score</th>\n","      <th>neg_score</th>\n","      <th>total_clean_words</th>\n","      <th>polarity_score</th>\n","      <th>sub_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>kill 10 million people decade highly infectiou...</td>\n","      <td>81</td>\n","      <td>38</td>\n","      <td>1037</td>\n","      <td>0.361345</td>\n","      <td>0.114754</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>human mind fascination carrying potential tink...</td>\n","      <td>78</td>\n","      <td>39</td>\n","      <td>614</td>\n","      <td>0.333333</td>\n","      <td>0.190554</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ai rapidly evolving employment sector matter i...</td>\n","      <td>70</td>\n","      <td>43</td>\n","      <td>867</td>\n","      <td>0.238938</td>\n","      <td>0.130334</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  pos_score  neg_score  \\\n","0  kill 10 million people decade highly infectiou...         81         38   \n","1  human mind fascination carrying potential tink...         78         39   \n","2  ai rapidly evolving employment sector matter i...         70         43   \n","\n","   total_clean_words  polarity_score  sub_score  \n","0               1037        0.361345   0.114754  \n","1                614        0.333333   0.190554  \n","2                867        0.238938   0.130334  "]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["df_scores.head(3)"]},{"cell_type":"markdown","id":"8d8a475c","metadata":{"id":"8d8a475c"},"source":["## Analysis of Readability"]},{"cell_type":"code","execution_count":null,"id":"b44721e6","metadata":{"id":"b44721e6"},"outputs":[],"source":["df_word=df_scores.copy()"]},{"cell_type":"code","execution_count":null,"id":"ae1005f5","metadata":{"id":"ae1005f5"},"outputs":[],"source":["# No of words before clean\n","\n","df_word['no_of_words_before_clean']=ar['text'].apply(lambda x: len(x.split()))"]},{"cell_type":"code","execution_count":null,"id":"5c1d3af2","metadata":{"id":"5c1d3af2"},"outputs":[],"source":["# sentence count\n","\n","import textstat\n","\n","df_word['sentence_count'] = ar['text'].apply(textstat.sentence_count)"]},{"cell_type":"code","execution_count":null,"id":"079adedb","metadata":{"id":"079adedb"},"outputs":[],"source":["# count complex words\n","\n","def count_complex_words(text):\n","    words = text.split()\n","    complex_words = [word for word in words if textstat.syllable_count(word) > 2]\n","    return len(complex_words)\n","\n","df_word['complex_words']=ar['text'].apply(lambda x:count_complex_words(x))"]},{"cell_type":"code","execution_count":null,"id":"29da5029","metadata":{"id":"29da5029"},"outputs":[],"source":["# counting syllable words \n","\n","def syllables_count(word):\n","    #referred from stackoverflow.com/questions/14541303/count-the-number-of-syllables-in-a-word\n","    count = 0\n","    word=word.lower()\n","    vowels = 'aeiouy'\n","    if word[0] in vowels:\n","        count +=1\n","    for index in range(1,len(word)):\n","        if word[index] in vowels and word[index-1] not in vowels:\n","            count +=1\n","    if word.endswith('e'):\n","        count -= 1\n","    if word.endswith('es'):\n","        count-=1\n","    if word.endswith('ed'):\n","        count-=1\n","    if count == 0:\n","        count += 1\n","    return count\n","df_word['syllable_words']=ar['text'].apply(lambda x:syllables_count(x))"]},{"cell_type":"code","execution_count":null,"id":"ab896c60","metadata":{"id":"ab896c60"},"outputs":[],"source":["# count Syllables per word\n","\n","import pyphen\n","# create a Pyphen object for the English language\n","dic = pyphen.Pyphen(lang='en')\n","\n","\n","def count_syllables(word):\n","    return len(dic.inserted(word).split('-'))\n","\n","df_word['syllables_per_word'] = ar['text'].apply(lambda x: sum([count_syllables(word) for word in x.lower().split()]))"]},{"cell_type":"code","execution_count":null,"id":"dc8a57d4","metadata":{"id":"dc8a57d4"},"outputs":[],"source":["# Personal Pronouns\n","\n","import re\n","pattern = re.compile(r'\\b(?:I|we|my|ours|us)\\b', flags=re.IGNORECASE)\n","\n","df_word['personal pronouns'] = [pattern.findall(text) for text in ar['text']]"]},{"cell_type":"code","execution_count":null,"id":"cf84570f","metadata":{"id":"cf84570f"},"outputs":[],"source":["# Average number of words per sentence\n","\n","sentences = ar['text'].apply(nltk.sent_tokenize)\n","\n","word_counts = sentences.apply(lambda x: [len(nltk.word_tokenize(sentence)) for sentence in x])\n","\n","total_words = word_counts.sum()\n","total_sentences = word_counts.apply(len).sum()\n","\n","avg_words_per_sentence  = total_words / total_sentences\n","\n","df_word['avg_no_of_words_per_Sentence']=df_word['no_of_words_before_clean']/ df_word['sentence_count']"]},{"cell_type":"code","execution_count":null,"id":"942f58c2","metadata":{"id":"942f58c2"},"outputs":[],"source":["# Average sentence length\n","import numpy as np\n","def get_avg_sentence_length(text):\n","    sentences = nltk.sent_tokenize(text)\n","    lengths = [len(sent.split()) for sent in sentences]\n","    return np.mean(lengths)\n","\n","df_word['avg_sent_length'] = ar['text'].apply(get_avg_sentence_length)"]},{"cell_type":"code","execution_count":null,"id":"7f4ca948","metadata":{"id":"7f4ca948"},"outputs":[],"source":["# Percentage of Complex words = the number of complex words / the number of words \n","\n","df_word['%_of_complex_words']=df_word['complex_words']/df_word['no_of_words_before_clean']"]},{"cell_type":"code","execution_count":null,"id":"e0d6ab9b","metadata":{"id":"e0d6ab9b"},"outputs":[],"source":["# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n","\n","df_word['fog_index']=0.4 * (df_word['avg_sent_length'] + df_word['%_of_complex_words'])"]},{"cell_type":"code","execution_count":null,"id":"a547db2a","metadata":{"id":"a547db2a"},"outputs":[],"source":["# Average number of words\n","\n","df_word['char_count'] = ar['text'].str.len()\n","\n","df_word['avg_word_length'] = df_word['char_count'] / df_word['no_of_words_before_clean']"]},{"cell_type":"markdown","id":"82d49139","metadata":{"id":"82d49139"},"source":["### Final Output Structure"]},{"cell_type":"code","execution_count":null,"id":"aaadce2a","metadata":{"id":"aaadce2a"},"outputs":[],"source":["df_word.loc[:,['URL_ID','URL']]=ar.loc[:,['URL_ID','URL']]"]},{"cell_type":"code","execution_count":null,"id":"c7445835","metadata":{"id":"c7445835"},"outputs":[],"source":["df_word=df_word.loc[:,['URL_ID','URL','pos_score','neg_score','polarity_score','sub_score','avg_sent_length','%_of_complex_words',\n","              'fog_index','avg_no_of_words_per_Sentence','complex_words','total_clean_words','syllables_per_word',\n","              'personal pronouns','avg_word_length']]"]},{"cell_type":"code","execution_count":null,"id":"4c80fc54","metadata":{"id":"4c80fc54"},"outputs":[],"source":["# rename columns names\n","df_word.rename(columns={'URL_ID':'URL_ID','URL':'URL','pos_score':'POSITIVE SCORE','neg_score':'NEGATIVE SCORE','polarity_score'\n","                       'sub_score':'SUBJECTIVITY SCORE','avg_sent_length':'AVERAGE SENTENCE LENGTH','%_of_complex_words':'PERCENTAGE OF COMPLEX WORDS',\n","                       'fog_index':'FOG INDEX','avg_no_of_words_per_Sentence':'AVERAGE NUMBER OF WORDS PER SENTENCE','complex_words':'COMPLEX WORDS',\n","                       'total_clean_words':'WORD COUNT','syllables_per_word':'SYLLABLES PER WORD','personal pronouns':'PERSONAL PRONOUNS',\n","                       'avg_word_length':'AVERAGE WORD LENGTH'},inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"c9911f51","metadata":{"id":"c9911f51","outputId":"41adbb81-1462-4593-8f04-38065aa679b1"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>URL_ID</th>\n","      <th>URL</th>\n","      <th>POSITIVE SCORE</th>\n","      <th>NEGATIVE SCORE</th>\n","      <th>polarity_score</th>\n","      <th>SUBJECTIVITY SCORE</th>\n","      <th>AVERAGE SENTENCE LENGTH</th>\n","      <th>PERCENTAGE OF COMPLEX WORDS</th>\n","      <th>FOG INDEX</th>\n","      <th>AVERAGE NUMBER OF WORDS PER SENTENCE</th>\n","      <th>COMPLEX WORDS</th>\n","      <th>WORD COUNT</th>\n","      <th>SYLLABLES PER WORD</th>\n","      <th>PERSONAL PRONOUNS</th>\n","      <th>AVERAGE WORD LENGTH</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37</td>\n","      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n","      <td>81</td>\n","      <td>38</td>\n","      <td>0.361345</td>\n","      <td>0.114754</td>\n","      <td>23.394737</td>\n","      <td>0.207537</td>\n","      <td>9.440909</td>\n","      <td>22.506329</td>\n","      <td>369</td>\n","      <td>1037</td>\n","      <td>3070</td>\n","      <td>[us]</td>\n","      <td>6.793026</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>38</td>\n","      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n","      <td>78</td>\n","      <td>39</td>\n","      <td>0.333333</td>\n","      <td>0.190554</td>\n","      <td>17.734177</td>\n","      <td>0.133476</td>\n","      <td>7.147061</td>\n","      <td>18.194805</td>\n","      <td>187</td>\n","      <td>614</td>\n","      <td>2064</td>\n","      <td>[we, us, us, i, us, we, we]</td>\n","      <td>5.997859</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>39</td>\n","      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n","      <td>70</td>\n","      <td>43</td>\n","      <td>0.238938</td>\n","      <td>0.130334</td>\n","      <td>20.083333</td>\n","      <td>0.200356</td>\n","      <td>8.113476</td>\n","      <td>20.083333</td>\n","      <td>338</td>\n","      <td>867</td>\n","      <td>2790</td>\n","      <td>[We, us, us]</td>\n","      <td>6.529935</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>40</td>\n","      <td>https://insights.blackcoffer.com/will-machine-...</td>\n","      <td>75</td>\n","      <td>33</td>\n","      <td>0.388889</td>\n","      <td>0.151685</td>\n","      <td>17.414894</td>\n","      <td>0.139890</td>\n","      <td>7.021913</td>\n","      <td>17.602151</td>\n","      <td>229</td>\n","      <td>712</td>\n","      <td>2399</td>\n","      <td>[us, we, us, we, we, we, we, we, we, we, US, w...</td>\n","      <td>5.981063</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41</td>\n","      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n","      <td>68</td>\n","      <td>30</td>\n","      <td>0.387755</td>\n","      <td>0.116114</td>\n","      <td>22.102564</td>\n","      <td>0.157773</td>\n","      <td>8.904135</td>\n","      <td>20.282353</td>\n","      <td>272</td>\n","      <td>844</td>\n","      <td>2691</td>\n","      <td>[us, We, we, we, we, we, us, us, we, we, We, W...</td>\n","      <td>6.255800</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   URL_ID                                                URL  POSITIVE SCORE  \\\n","0      37  https://insights.blackcoffer.com/ai-in-healthc...              81   \n","1      38  https://insights.blackcoffer.com/what-if-the-c...              78   \n","2      39  https://insights.blackcoffer.com/what-jobs-wil...              70   \n","3      40  https://insights.blackcoffer.com/will-machine-...              75   \n","4      41  https://insights.blackcoffer.com/will-ai-repla...              68   \n","\n","   NEGATIVE SCORE  polarity_score  SUBJECTIVITY SCORE  \\\n","0              38        0.361345            0.114754   \n","1              39        0.333333            0.190554   \n","2              43        0.238938            0.130334   \n","3              33        0.388889            0.151685   \n","4              30        0.387755            0.116114   \n","\n","   AVERAGE SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n","0                23.394737                     0.207537   9.440909   \n","1                17.734177                     0.133476   7.147061   \n","2                20.083333                     0.200356   8.113476   \n","3                17.414894                     0.139890   7.021913   \n","4                22.102564                     0.157773   8.904135   \n","\n","   AVERAGE NUMBER OF WORDS PER SENTENCE  COMPLEX WORDS  WORD COUNT  \\\n","0                             22.506329            369        1037   \n","1                             18.194805            187         614   \n","2                             20.083333            338         867   \n","3                             17.602151            229         712   \n","4                             20.282353            272         844   \n","\n","   SYLLABLES PER WORD                                  PERSONAL PRONOUNS  \\\n","0                3070                                               [us]   \n","1                2064                        [we, us, us, i, us, we, we]   \n","2                2790                                       [We, us, us]   \n","3                2399  [us, we, us, we, we, we, we, we, we, we, US, w...   \n","4                2691  [us, We, we, we, we, we, us, us, we, we, We, W...   \n","\n","   AVERAGE WORD LENGTH  \n","0             6.793026  \n","1             5.997859  \n","2             6.529935  \n","3             5.981063  \n","4             6.255800  "]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["df_word.head()"]},{"cell_type":"code","execution_count":null,"id":"a852e662","metadata":{"id":"a852e662"},"outputs":[],"source":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}
